import os, requests, json, re
from threading import Thread
from typing import Iterator, List, Tuple
from transformers import AutoTokenizer

import gradio as gr
from gradio_client import Client

username = os.getenv('USERNAME')
password = os.getenv('PASSWORD')
RA_client = Client("http://rag-backend-server:5004/api/", auth=(username, password))
# LLM_client = Client("http://hf-llm-chatbot:5002/chat", auth=(username, password))

model_id = "Viet-Mistral/Vistral-7B-Chat"
tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir='/hf_cache')
tokenizer.use_default_system_prompt = False

MAX_NEW_TOKENS = 4096
DEFAULT_MAX_NEW_TOKENS = 1024
MAX_INPUT_TOKEN_LENGTH = int(os.getenv("MAX_INPUT_TOKEN_LENGTH", "2048"))
SYS_PROMPT = "B·∫°n l√† m·ªôt tr·ª£ l√Ω t∆∞ v·∫•n ph√°p l√Ω v·ªÅ c√°c v·∫•n ƒë·ªÅ Lu·∫≠t v√† Quy chu·∫©n - Ti√™u chu·∫©n. "\
             "H√£y suy lu·∫≠n ƒë·ªÉ ph·∫£n h·ªìi c√°c y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n t√†i li·ªáu ho·∫∑c th√¥ng tin ƒë∆∞·ª£c cung c·∫•p (n·∫øu c√≥). " \
             "N·∫øu m·ªôt c√¢u h·ªèi kh√¥ng c√≥ √Ω nghƒ©a ho·∫∑c kh√¥ng h·ª£p l√Ω v·ªÅ m·∫∑t th√¥ng tin, h√£y gi·∫£i th√≠ch t·∫°i sao thay v√¨ tr·∫£ l·ªùi m·ªôt ƒëi·ªÅu g√¨ ƒë√≥ kh√¥ng ch√≠nh x√°c. " \
             "N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi cho m·ªôt c√¢u h·ªèi ho·∫∑c kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan, h√£y tr·∫£ l·ªùi l√† b·∫°n kh√¥ng bi·∫øt v√† vui l√≤ng kh√¥ng chia s·∫ª th√¥ng tin sai l·ªách."
             
def prompt_format(system_prompt, instruction):
    prompt = f"""{system_prompt}
            ####### Instruction:
            {instruction}
            %%%%%%% Response:
            """
    return prompt

def generate(
    message: str,
    chat_history: List[Tuple[str, str]],
    system_prompt: str,
    max_new_tokens: int = 1024,
    temperature: float = 0.6,
    top_p: float = 0.9,
    top_k: int = 50,
    repeat_penalty: float = 1.2,
    repeat_last_n: int = 64,
) -> Iterator[str]:
    conversation = []
    if system_prompt:
        conversation.append({"role": "system", "content": system_prompt})
    ra_outputs = RA_client.predict(query=message, expand_query=True, api_name="/search")
    context = "\n\n".join([item['_doc'] for item in ra_outputs])
    query = "B·∫°n h√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a v√†o th√¥ng tin sau:\n" + context + "\nC√¢u h·ªèi:" + message
    conversation.append({"role": "user", "content": query})
    prompt = tokenizer.apply_chat_template(conversation, tokenize=False) \
                        .replace(tokenizer.bos_token, '') \
                        .replace(tokenizer.eos_token, '')
    headers = {
        'Content-Type': 'application/json',
        'Authorization':  f"Bearer {os.getenv('LLM_SERVER_API_KEY')}"
    }
    json_data = {
        "prompt": prompt, 
        "n_predict": max_new_tokens,
        "seed": -1,
        "temp": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "logit_bias": os.getenv('LOGIT_BIAS'),
        "repeat_penalty": repeat_penalty,
        "repeat_last_n": repeat_last_n,
        "stream": True
    }
    
    response = requests.post('http://llm-inference-platform-cuda:8080/completion', 
                             headers=headers, 
                             json=json_data, 
                             stream=True)

    outputs = []
    for chunk in response.iter_content(chunk_size=1024):
        if chunk:
            try:
                data = re.search(r'\{.*\}', chunk.decode('utf-8', 'ignore')).group()
                text = json.loads(data)['content']
                outputs.append(text)
                yield "".join(outputs)
            except Exception as e:
                print(chunk.decode('utf-8', 'ignore'), e.args)
                
                

chat_interface = gr.ChatInterface(
    fn=generate,
    additional_inputs=[
        gr.Textbox(label="System prompt", lines=6, value=SYS_PROMPT),
        gr.Slider(
            label="Max new tokens",
            minimum=1,
            maximum=MAX_NEW_TOKENS,
            step=1,
            value=DEFAULT_MAX_NEW_TOKENS,
        ),
        gr.Slider(
            label="Temperature",
            minimum=0.1,
            maximum=4.0,
            step=0.1,
            value=0.3,
        ),
        gr.Slider(
            label="Top-p (nucleus sampling)",
            minimum=0.05,
            maximum=1.0,
            step=0.05,
            value=0.9,
        ),
        gr.Slider(
            label="Top-k",
            minimum=1,
            maximum=1000,
            step=1,
            value=50,
        ),
        gr.Slider(
            label="Repetition penalty",
            minimum=1.0,
            maximum=2.0,
            step=0.05,
            value=1.05,
        ),        
        gr.Slider(
            label="Repetition last N tokens",
            minimum=-1,
            maximum=400,
            step=1,
            value=100,
        ),
    ],
    examples=[
        ["Nguy√™n t·∫Øc trong giao th√¥ng ƒë∆∞·ªùng s·∫Øt l√† g√¨?"],
        ["Quy ƒë·ªãnh m·ªõi nh·∫•t v·ªÅ ƒëƒÉng ki·ªÉm ph∆∞∆°ng ti·ªán ƒë∆∞·ªùng thu·ª∑ n·ªôi ƒë·ªãa?"],
        ["Quy tr√¨nh ƒëƒÉng k√Ω xe c∆° gi·ªõi"],
    ],
    submit_btn=gr.Button("üí¨ G·ª≠i"),
    retry_btn=gr.Button("üîÑ Th·ª≠ l·∫°i"),
    stop_btn=gr.Button("‚õî D·ª´ng"),
    clear_btn=gr.Button("ü™Ñ X√≥a"),
    undo_btn=gr.Button("üí¢ Quay l·∫°i"),
)

with gr.Blocks(css="style.css", title='VR-Chatbot') as demo:
    gr.Markdown('# Vietnam Register Chatbot')
    # gr.DuplicateButton(value="Duplicate Space for private use", elem_id="duplicate-button")
    chat_interface.render()
    # for component in chat_interface.additional_inputs:
    #     component.visible = False
    # gr.Markdown(LICENSE)
